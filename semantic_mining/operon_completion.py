"""
Operon completion eval pipeline using Evo.

Usage: python operon_completion.py --config <config_file_path>
"""

import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, NamedTuple, Union
from dataclasses import dataclass
import logging
import json
import os
import pandas as pd
import tempfile
from Bio import SeqIO, AlignIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from collections import Counter
import subprocess

from semantic_mining import (
    read_prompts,
    model_load,
    sample_model,
    get_rc,
    make_fasta,
    run_prodigal,
    filter_protein_fasta,
)

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class ModelOutput(NamedTuple):
    sequences: List[str]
    scores: List[float]


class BatchData(NamedTuple):
    prompts: List[str]
    sequences: List[str]
    scores: List[str]
    ids: List[str]


@dataclass
class SequenceResult:
    """Data class to store sequence analysis results."""

    UUID: str
    Generated_Sequence: str
    Prompt: str
    Expected_Response: str
    Reference_Sequence: str
    Sequence_Identity: float


@dataclass
class Config:
    """Configuration for protein sequence generation and analysis."""

    # Input/Output paths
    input_prompts: str  # CSV containing input prompts
    evo_gen_seqs_file_save_location: (
        str  # CSV continaing sequences generated by Evo and corresponding prompts
    )
    all_seqs_fasta: str  # FASTA containing Evo generated sequences used as input to Prodigal
    proteins_file: str  # FASTA containing all prodigal-detected protein sequences
    orfs_file: str  # FASTA containing all prodigal-detected ORFs
    filtered_proteins_file: str  # FASTA containing all protein sequences post-complexity and length filtering
    reference_seqs: str  # FASTA containing all reference protein sequences if aligment against reference sequences is desired
    msa_filtered_proteins_fasta: (
        str  # FASTA containing protein sequences that pass aligment against reference sequences
    )
    sequence_alignment_csv: str  # CSV containing all alignments for alignment against references
    output_msa_csv: Path  # CSV containing all alignments for alignment against references
    output_summary_csv: (
        Path  # CSV containing summary statistics about overall sequence identities for all prompts
    )
    segmasker_path: Path  # Path to Segmasker installation
    mafft_path: Path  # Path to MAFFT installation

    # Model parameters
    model_name: str  # Model name to use for sampling
    n_tokens: int  # Number of tokens of sequence to be generated
    temperature: float  # Temperature to sample at
    top_k: int  # Top k value for sampling
    batched: bool  # If generations should be batched (recommend setting to true for quicker generation)
    batch_size: int  # Size of batch for batched generation
    n_sample_per_prompt: int  # Number of times a sequence should be generated for a given prompt (e.g., 3 means sample 3 times on a given prompt)

    # Filter parameters
    rc_truth: bool  # True if the reverse complement of the generated sequence should be included when downstream processing
    return_both: bool  # True if both the reverse complement and original generated sequence should be used for downstream processing
    filter_min_length: int  # Minimum length of protein to keep during filtering
    filter_max_length: int  # Maximum length of protein to keep during filtering
    filter_partial_bool: bool  # True if partial ORFs should be removed during downstream filtering, only set to True if -p meta flag is not used
    segmasker_threshold: float  # Proportion of protein sequence that can be low-complexity for sequence to be kept during filtering
    run_msa: bool  # True if MSA against a list of reference sequences should be run during filtering
    seq_identity_match_threshold: float  # If running MSA, minimum sequence identity that must exist between generated protein sequence and any reference sequence


def calculate_sequence_identity(seq1: str, seq2: str, mafft_path: str = "mafft") -> Optional[float]:
    """
    Calculate sequence identity between two sequences using MAFFT.

    Args:
        seq1: First amino acid sequence
        seq2: Second amino acid sequence
        mafft_path: Path to MAFFT executable (default: "mafft")

    Returns:
        Percent identity between sequences (0-100), or None if alignment fails
    """
    if not seq1 or not seq2:
        return None

    # Create SeqRecord objects
    record1 = SeqRecord(Seq(seq1), id="seq1")
    record2 = SeqRecord(Seq(seq2), id="seq2")

    try:
        # Run alignment
        aligned_seq1, aligned_seq2, identity = align_pair(record1, record2, mafft_path)
        return identity * 100

    except subprocess.CalledProcessError as e:
        print(f"MAFFT alignment failed: {str(e)}")
        return None


def align_pair(query_record: SeqRecord, ref_record: SeqRecord, mafft_path: str) -> Tuple[str, str, float]:
    """
    Align a pair of sequences using MAFFT.

    Args:
        query_record: First sequence as SeqRecord
        ref_record: Second sequence as SeqRecord
        mafft_path: Path to MAFFT executable

    Returns:
        Tuple containing:
            - Aligned first sequence (string)
            - Aligned second sequence (string)
            - Sequence identity (float between 0-1)
    """
    aligned_file_name = None
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".fasta") as tmp_fasta:
        SeqIO.write([query_record, ref_record], tmp_fasta, "fasta")
        tmp_fasta_name = tmp_fasta.name

    try:
        result = subprocess.run([mafft_path, tmp_fasta_name], capture_output=True, text=True, check=True)
        with tempfile.NamedTemporaryFile(mode="w+", delete=False) as aligned_file:
            aligned_file.write(result.stdout)
            aligned_file_name = aligned_file.name

        alignment = AlignIO.read(aligned_file_name, "fasta")
        aligned_seq1, aligned_seq2 = alignment[0].seq, alignment[1].seq

        identity_count = sum(
            1 for a, b in zip(aligned_seq1, aligned_seq2) if a != "-" and b != "-" and a == b
        )
        aligned_length = sum(1 for a, b in zip(aligned_seq1, aligned_seq2) if a != "-" and b != "-")
        identity = identity_count / aligned_length if aligned_length > 0 else 0

        return str(aligned_seq1), str(aligned_seq2), identity

    finally:
        try:
            os.remove(tmp_fasta_name)
        except OSError:
            pass

        if aligned_file_name:
            try:
                os.remove(aligned_file_name)
            except OSError:
                pass


def align_and_save_closest_match(
    input_fasta: Path,
    reference_fasta: Path,
    output_csv: Path,
    filtered_fasta: Path,
    identity_threshold: float,
    mafft_path: str = "mafft",
) -> None:
    """
    Align sequences and save closest matches using MAFFT.

    Args:
        input_fasta: Path to input FASTA file containing query sequences
        reference_fasta: Path to reference FASTA file containing sequences to match against
        output_csv: Path to save alignment results CSV
        filtered_fasta: Path to save filtered sequences FASTA
        identity_threshold: Minimum percent identity threshold for keeping matches (0-100)
        mafft_path: Path to MAFFT executable (default: "mafft")

    Generated Files:
        output_csv: CSV file with columns:
            - query_id: ID of query sequence
            - reference_id: ID of best matching reference sequence
            - identity: Percent identity between query and reference

        filtered_fasta: FASTA file containing query sequences that had matches
            above the identity threshold
    """
    reference_seqs = {record.id: record for record in SeqIO.parse(reference_fasta, "fasta")}

    results = []
    filtered_records = []

    for record in SeqIO.parse(input_fasta, "fasta"):
        best_identity = 0.0
        best_match = None

        for ref_id, ref_record in reference_seqs.items():
            _, _, identity = align_pair(record, ref_record, mafft_path)
            identity *= 100
            if identity > best_identity:
                best_identity = identity
                best_match = ref_id

        if best_identity >= identity_threshold:
            results.append({"query_id": record.id, "reference_id": best_match, "identity": best_identity})
            filtered_records.append(record)

    pd.DataFrame(results).to_csv(output_csv, index=False)
    SeqIO.write(filtered_records, filtered_fasta, "fasta")


def create_summary_statistics(results_df: pd.DataFrame, output_path: Path) -> None:
    """
    Create summary statistics for operon completion task.

    Args:
        results_df: DataFrame containing sequence analysis results with columns:
            - Prompt: The input prompt used to generate sequence
            - Expected_Response: Expected sequence/pattern
            - Sequence_Identity: Percent identity from alignment
        output_path: Path to save summary statistics CSV

    Generated Files:
        output_path: CSV file containing summary statistics with columns:
            - Prompt: The input prompt
            - Expected_Response: The expected sequence pattern
            - avg_identity: Mean sequence identity for the prompt-response pair
            - std_identity: Standard deviation of sequence identities
            - count: Number of sequences for this prompt-response pair
    """
    if results_df.empty:
        logger.error("No results to process. Exiting.")
        return

    summary_stats = (
        results_df.groupby(["Prompt", "Expected_Response"])
        .agg({"Sequence_Identity": ["mean", "std", "count"]})
        .reset_index()
    )

    summary_stats.columns = ["Prompt", "Expected_Response", "avg_identity", "std_identity", "count"]

    numeric_cols = ["avg_identity", "std_identity"]
    summary_stats[numeric_cols] = summary_stats[numeric_cols].round(2)

    summary_stats.to_csv(output_path, index=False)


def process_operon_sequences(
    input_fasta: Path,
    uuid_prompts_csv: Path,
    prompt_info_csv: Path,
    reference_fasta: Path,
    output_msa_csv: Path,
    output_summary_csv: Path,
    mafft_path: Path,
) -> None:
    """
     Args:
        input_fasta: FASTA file containing generated sequences with UUID headers
        uuid_prompts_csv: CSV mapping UUIDs to prompts with columns:
            - UUID: Unique identifier for each sequence
            - Prompt: The prompt used to generate the operon gene
        prompt_info_csv: CSV mapping prompts to expected gene responses with columns:
            - Prompt: The input prompt
            - Expected_Response: Expected gene sequence name/ID
        reference_fasta: FASTA file containing reference gene sequences
        output_msa_csv: Path to save MSA results CSV
        output_summary_csv: Path to save summary statistics CSV

    Generated Files:
        output_msa_csv: CSV file containing MSA and identity analysis with columns:
            - UUID: Unique identifier for generated operon
            - Generated_Sequence: The operon sequence generated by the model
            - Prompt: Input prompt used to generate operon
            - Expected_Response: Expected operon sequence ID
            - Reference_Sequence: The reference operon sequence
            - Sequence_Identity: Percent identity between generated and reference operons

        output_summary_csv: CSV file with summary statistics grouped by prompt containing:
            - Mean sequence identity
            - Standard deviation
            - Number of samples per prompt
    """
    logger.info("Starting sequence analysis...")

    input_sequences: Dict[str, str] = {}
    for record in SeqIO.parse(input_fasta, "fasta"):
        uuid = record.id.split("_")[0]
        sequence = str(record.seq).replace("*", "")
        input_sequences[uuid] = sequence
        logger.info(f"Loaded sequence for UUID: {uuid}")

    uuid_prompt_map = pd.read_csv(uuid_prompts_csv)

    prompt_response_map = pd.read_csv(prompt_info_csv)

    reference_sequences: Dict[str, str] = {}
    for record in SeqIO.parse(reference_fasta, "fasta"):
        reference_sequences[record.id] = str(record.seq)
        logger.info(f"Loaded reference sequence: {record.id}")

    results: List[SequenceResult] = []

    for uuid, generated_seq in input_sequences.items():
        try:
            prompt_match = uuid_prompt_map[uuid_prompt_map["UUID"] == uuid]
            if prompt_match.empty:
                logger.warning(f"No matching prompt found for UUID: {uuid}")
                continue
            prompt = prompt_match["Prompt"].iloc[0]

            response_match = prompt_response_map[prompt_response_map["Prompt"] == prompt]
            if response_match.empty:
                logger.warning(f"No matching expected response found for prompt: {prompt}")
                continue
            expected_response = response_match["Expected_Response"].iloc[0]

            reference_seq = reference_sequences.get(expected_response)
            if not reference_seq:
                logger.warning(f"No reference sequence found for expected response: {expected_response}")
                continue

            sequence_identity = calculate_sequence_identity(generated_seq, reference_seq, mafft_path)
            logger.info(f"Calculated sequence identity for {uuid}: {sequence_identity}%")

            result = SequenceResult(
                UUID=uuid,
                Generated_Sequence=generated_seq,
                Prompt=prompt,
                Expected_Response=expected_response,
                Reference_Sequence=reference_seq,
                Sequence_Identity=sequence_identity,
            )
            results.append(result)

        except Exception as e:
            logger.error(f"Error processing UUID {uuid}: {str(e)}")
            continue

    output_df = pd.DataFrame([vars(r) for r in results])
    output_df.to_csv(output_msa_csv, index=False)
    logger.info(f"Results saved to {output_msa_csv}")

    create_summary_statistics(output_df, output_summary_csv)
    logger.info(f"Summary statistics saved to {output_summary_csv}")


def run_pipeline(config_file: str) -> None:
    """
    Main pipeline function for running operon completion evaluation.

    Args:
        config_file: Path to JSON configuration file containing pipeline settings

    Generated Files:
        - Generated sequences FASTA
        - Protein predictions from Prodigal
        - Filtered protein sequences
        - Multiple sequence alignments of generated proteins and reference sequences
        - Summary statistics CSV

    """

    with open(config_path) as f:
        config_dict = json.load(f)
    config = Config(**config_dict)

    prompt_seqs = read_prompts(config.input_prompts, config.batched, config.batch_size)
    model, tokenizer = model_load(config.model_name)

    batch_data = sample_model(
        prompt_batches=prompt_seqs,
        model=model,
        tokenizer=tokenizer,
        file_save_location=config.evo_gen_seqs_file_save_location,
        n_tokens=config.n_tokens,
        temp=config.temperature,
        top_k=config.top_k,
        batched=config.batched,
        n_sample_per_prompt=config.n_sample_per_prompt,
        force_prompt_threshold=2,
    )
    prompts, sequences, scores, ids = batch_data

    final_sequences = get_rc(sequences, rc_truth=config.rc_truth, return_both=config.return_both)
    make_fasta(final_sequences, prompts, ids, config.all_seqs_fasta)

    run_prodigal(config.all_seqs_fasta, config.proteins_file, config.orfs_file)
    filter_protein_fasta(
        config.proteins_file,
        config.filtered_proteins_file,
        config.segmasker_path,
        config.filter_min_length,
        config.filter_max_length,
        config.filter_partial_bool,
        config.segmasker_threshold,
    )
    align_and_save_closest_match(
        config.filtered_proteins_file,
        config.reference_seqs,
        config.sequence_alignment_csv,
        config.msa_filtered_proteins_fasta,
        config.seq_identity_match_threshold,
        config.mafft_path,
    )
    process_operon_sequences(
        input_fasta=config.msa_filtered_proteins_fasta,
        uuid_prompts_csv=config.evo_gen_seqs_file_save_location,
        prompt_info_csv=config.input_prompts,
        reference_fasta=config.reference_seqs,
        output_msa_csv=config.output_msa_csv,
        output_summary_csv=config.output_summary_csv,
        mafft_path=config.mafft_path,
    )

    logger.info("Pipeline execution completed successfully")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run sampling script with a configuration file.")
    parser.add_argument(
        "--config", required=True, help="Path to the configuration file (e.g., path/to/config.json)"
    )
    args = parser.parse_args()
    run_pipeline(args.config)
