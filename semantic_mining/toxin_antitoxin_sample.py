"""
Toxin-antitoxin sampling pipeline using Evo.

Usage: python toxin_antitoxin_sample.py --config <config_file_path>
"""

import argparse
import json
import pandas as pd
from transformers import (
    AutoTokenizer,
    EsmForProteinFolding,
)
from dataclasses import dataclass
from typing import Dict, Any
from semantic_mining import (
    read_prompts,
    model_load,
    sample_model,
    get_rc,
    make_fasta,
    run_prodigal,
    filter_protein_fasta,
    fold_proteins,
    filter_proteins_by_threshold,
)
from stripedhyena.model import StripedHyena
from stripedhyena.tokenizer import CharLevelTokenizer


@dataclass
class Config:
    """Configuration for protein sequence generation and analysis."""

    # Input/Output paths
    input_prompts: str  # CSV containing input prompts
    evo_gen_seqs_file_save_location: (
        str  # CSV continaing sequences generated by Evo and corresponding prompts
    )
    all_seqs_fasta: str  # FASTA containing Evo generated sequences used as input to Prodigal
    proteins_file: str  # FASTA containing all prodigal-detected protein sequences
    orfs_file: str  # FASTA containing all prodigal-detected ORFs
    filtered_proteins_file: str  # FASTA containing all protein sequences post-complexity and length filtering
    segmasker_path: str  # Path to Segmasker installation
    output_folds_file: (
        str  # CSV containing folds and sequence information for all protein sequences folded using ESMFold
    )
    output_filtered_folds: str  # CSV containing sequences that passed fold quality filtering
    shared_protein_pairs_csv: str  # CSV containing all pairs of proteins in a single generation
    cofold_fasta: str  # FASTA containing sequences of paired proteins to use as input for cofolding

    # Model parameters
    model_name: str  # Model name to use for sampling
    n_tokens: int  # Number of tokens of sequence to be generated
    temperature: float  # Temperature to sample at
    top_k: int  # Top k value for sampling
    batched: bool  # If generations should be batched (recommend setting to true for quicker generation)
    batch_size: int  # Size of batch for batched generation
    n_sample_per_prompt: int  # Number of times a sequence should be generated for a given prompt (e.g., 3 means sample 3 times on a given prompt)

    # Filter parameters
    rc_truth: bool  # True if the reverse complement of the generated sequence should be included when downstream processing
    return_both: bool  # True if both the reverse complement and original generated sequence should be used for downstream processing
    filter_min_length: int  # Minimum length of protein to keep during filtering
    filter_max_length: int  # Maximum length of protein to keep during filtering
    filter_partial_bool: bool  # True if partial ORFs should be removed during downstream filtering, , only set to True if -p meta flag is not used
    segmasker_threshold: float  # Proportion of protein sequence that can be low-complexity for sequence to be kept during filtering
    run_esm_fold: bool  # True if protein sequences should be folded during filtering
    plddt_threshold: float  # Minimum pLDDT value of folded protein sequence to keep during filtering
    ptm_threshold: float  # Minimum pTM value of folded protein sequence to keep during filtering

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> "Config":
        """Create Config instance from dictionary."""
        return cls(**config_dict)


def identify_unique_pairs(df: pd.DataFrame, output_csv: str) -> pd.DataFrame:
    """
    Identifies all unique pairs of proteins with the same root ID and labels them as 1 and 2.

    Args:
    df (pd.DataFrame): Input DataFrame with protein information.
    output_csv (str): Path to the output CSV file to save the unique pairs.

    Returns:
    pd.DataFrame: DataFrame containing unique pairs of proteins.
    """
    df["Root_ID"] = df["Evo Sequence ID"].str.extract(r"([A-Za-z0-9]+)_")

    grouped = df.groupby("Root_ID")

    unique_pairs = []
    for root_id, group in grouped:
        if len(group) > 1:
            sequences = group["Amino Acid Sequence"].tolist()
            seq_ids = group["Evo Sequence ID"].tolist()
            for i in range(len(sequences)):
                for j in range(i + 1, len(sequences)):
                    unique_pairs.append(
                        {
                            "Root_ID": root_id,
                            "Evo Sequence ID 1": seq_ids[i],
                            "Amino Acid Sequence 1": sequences[i],
                            "Evo Sequence ID 2": seq_ids[j],
                            "Amino Acid Sequence 2": sequences[j],
                        }
                    )

    unique_pairs_df = pd.DataFrame(unique_pairs)

    unique_pairs_df = unique_pairs_df.drop_duplicates()

    unique_pairs_df.to_csv(output_csv, index=False)

    return unique_pairs_df


def csv_to_cofold_fasta(
    input_csv: str,
    cofold_fasta: str,
    root_id_col: str = "Root_ID",
    sequence1_col: str = "Amino Acid Sequence 1",
    sequence2_col: str = "Amino Acid Sequence 2",
    sequence1_id_col: str = "Evo Sequence ID 1",
    sequence2_id_col: str = "Evo Sequence ID 2",
):
    """
    Processes a CSV file containing toxin-antitoxin pairs, converting it to a FASTA format
    with modified headers.

    Args:
        input_csv (str): Path to the input CSV file containing the fold-filtered generations
        output_fasta (str): Path to the output FASTA file containing all pairs of proteins to be co-folded
        root_id_col (str): Column name for the root ID descriptor
        sequence1_col (str): Column name for first protein sequence on generation
        seuqence2_col (str): Column name for second protein sequence on generation
        sequence1_id_col (str): Column name for ID of first protein sequence on generation (corresponding to ORF position on generation)
        sequence2_id_col (str): Column name for ID of second protein sequence on generation (corresponding to ORF position on generation)

    Files Generated:
        output_fasta: FASTA file with headers in format:
            >{root_id},{number_after_underscore_sequence1},{number_after_underscore_sequence2}
            {sequence1}:
            {sequence2}
    """
    try:
        df = pd.read_csv(input_csv)
        required_cols = [root_id_col, sequence1_col, sequence2_col, sequence1_id_col, sequence2_id_col]
        for col in required_cols:
            if col not in df.columns:
                raise KeyError(f"Column '{col}' not found in the CSV file.")

        unique_pairs = df[required_cols].drop_duplicates()
        seen_pairs = set()
        unique_rows = []

        for _, row in unique_pairs.iterrows():
            pair = (row[sequence1_col], row[sequence2_col])
            if pair not in seen_pairs:
                seen_pairs.add(pair)
                unique_rows.append(row)

        with open(cofold_fasta, "w") as fasta_file:
            for row in unique_rows:
                tox_number = row[sequence1_id_col].split("_")[1].split()[0]
                antitox_number = row[sequence2_id_col].split("_")[1].split()[0]
                root_id = row[root_id_col]

                header = f">{root_id},{tox_number},{antitox_number}"

                seq1 = row[sequence1_col]
                seq2 = row[sequence2_col]

                fasta_file.write(f"{header}\n{seq1}:\n{seq2}\n")

        print(f"Processing complete. Output written to '{cofold_fasta}'.")

    except FileNotFoundError as e:
        print(f"File not found: {str(e)}")
    except KeyError as e:
        print(f"Column error: {str(e)}")
    except Exception as e:
        print(f"An error occurred: {str(e)}")


def load_config(config_file: str) -> Config:
    """Load configuration from JSON file."""
    try:
        with open(config_file, "r") as f:
            config_dict = json.load(f)
        return Config.from_dict(config_dict)
    except FileNotFoundError:
        raise FileNotFoundError(f"Configuration file not found: {config_file}")
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON in configuration file: {config_file}")


def process_sequences(config: Config, model: StripedHyena, tokenizer: CharLevelTokenizer) -> None:
    """Process sequences through the pipeline."""
    print("Starting sequence processing...")

    # Read prompts
    prompt_seqs = read_prompts(config.input_prompts, config.batched, config.batch_size)
    print("Prompts loaded")

    # Generate sequences
    batch_data = sample_model(
        prompt_batches=prompt_seqs,
        model=model,
        tokenizer=tokenizer,
        file_save_location=config.evo_gen_seqs_file_save_location,
        n_tokens=config.n_tokens,
        temp=config.temperature,
        top_k=config.top_k,
        batched=config.batched,
        n_sample_per_prompt=config.n_sample_per_prompt,
        force_prompt_threshold=2,
    )
    prompts, sequences, scores, ids = batch_data
    # Process sequences
    final_sequences = get_rc(sequences, rc_truth=config.rc_truth, return_both=config.return_both)

    # Create FASTA files
    make_fasta(final_sequences, prompts, ids, config.all_seqs_fasta)

    # Run protein analysis pipeline
    run_prodigal(config.all_seqs_fasta, config.proteins_file, config.orfs_file)

    print("Base protein filtering started...", flush=True)
    filter_protein_fasta(
        config.proteins_file,
        config.filtered_proteins_file,
        config.segmasker_path,
        config.filter_min_length,
        config.filter_max_length,
        config.filter_partial_bool,
        config.segmasker_threshold,
    )
    print("Base protein filtering complete", flush=True)


def process_folds(config: Config) -> pd.DataFrame:
    """Process sequence alignments and protein folding."""
    print("Starting protein folding...", flush=True)
    fold_stats = fold_proteins(config.filtered_proteins_file, config.output_folds_file)
    print("Protein folding complete", flush=True)
    filtered_folds = filter_proteins_by_threshold(
        fold_stats, config.output_filtered_folds, config.plddt_threshold, config.ptm_threshold
    )

    return filtered_folds


def main(config_file: str) -> None:
    try:
        # Load configuration
        config = load_config(config_file)
        print("Configuration loaded", flush=True)

        model, tokenizer = model_load(config.model_name)

        # Execute pipeline
        process_sequences(config, model, tokenizer)
        if config.run_esm_fold:
            filtered_folds = process_folds(config)
            protein_pairs = identify_unique_pairs(filtered_folds, config.shared_protein_pairs_csv)
            csv_to_cofold_fasta(config.shared_protein_pairs_csv, config.cofold_fasta)
        print("Pipeline completed successfully", flush=True)
    except Exception as e:
        print(f"Error in pipeline execution: {str(e)}", flush=True)
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run sampling script with a configuration file.")
    parser.add_argument(
        "--config", required=True, help="Path to the configuration file (e.g., path/to/config.json)"
    )
    args = parser.parse_args()
    main(args.config)
